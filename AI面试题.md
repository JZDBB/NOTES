# 算法工程师面试

## 深度学习 

### 模型评估方法

#### Accuracy作为指标有哪些局限性？

> 当正负样本极度不均衡时存在问题！当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。

#### ROC曲线和PR曲线各是什么？

> 受试者工作特征曲线（Receiver Operating Characteristic，ROC）
> ROC关注两个指标：
> 1） True Positive Rate ( TPR ) = TP / [ TP + FN] ，TPR代表能将正例分对的概率
> 2）False Positive Rate( FPR ) = FP / [ FP + TN] ，FPR代表将负例错分为正例的概率
>
> PR（Precision - Recall）曲线 
> 准确率（Precision，P）：P = TP / (TP + FP)
> 召回率（Recall，R）：R = TP/ (TP + FN)
> **PR曲线实则是以precision（精准率）和recall（召回率）这两个为变量而做出的曲线，其中recall为横坐标，precision为纵坐标。**
> 平衡点（BEP）是P=R时的取值，如果这个值较大，则说明学习器的性能较好。而F1 = 2 * P * R ／( P + R )，同样，F1值越大，我们可以认为该学习器的性能较好。
> ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。
>
> True Positive, TP: 预测为正样本，实际也为正样本
> False Positive, FP: 预测为正样本，实际为负样本
> True Negative, TN: 预测为负样本，实际也为负样本
> False Negative, FN: 预测为负样本，实际为正样本

#### 编程实现AUC的计算，并指出复杂度？

> 有两种计算AUC的方法:
> 1.绘制ROC曲线，ROC曲线下面的面积就是AUC的值
> 2.假设总共有（m+n）个样本，其中正样本m个，负样本n个，总共有mn个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以（mn）就是AUC的值
> 
> ```python
> def calc_auc(y_labels, y_scores):
>  f = list(zip(y_scores, y_labels))
>  rank = [values2 for values1, values2 in sorted(f, key=lambda x:x[0])]
>  rankList = [i+1 for i in range(len(rank)) if rank[i] == 1]
>  pos_cnt = np.sum(y_labels == 1)
>  neg_cnt = np.sum(y_labels == 0)
>  auc = (np.sum(rankList) - pos_cnt*(pos_cnt+1)/2) / (pos_cnt*neg_cnt)
>  print(auc)
> ```
>
> 排序复杂度：$O(log2(P+N))$
>
> 计算AUC的复杂度：$O(P+N)$

#### AUC指标有什么特点？放缩结果对AUC是否有影响？

> **AUC (Area under Curve)：ROC曲线下的面积，作为数值可以直观的评价分类器的好坏，值越大越好。**
> $ AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
>  $0.5 < AUC < 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
>  $AUC = 0.5$，模型没有预测价值
>
> 放缩结果对$AUC$没有影响

#### 余弦距离与欧式距离有什么特点？

> 余弦距离$cos\theta=\frac{<x,y>}{||x||||y||}$
>
> 欧氏距离： $d(x,y)=\sqrt{\sum_{i=0}^N(x_i-y_i)^2}$
>
> 余弦距离，也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡畺两个个体间差异的大小的度量。如果两个向量的方向一致，即夹角接近零，那么这两个向量就相近。
>
> 余弦距离使用两个向量夹角的余弦值作为衡是两个个体间差异的大小。
>
> 相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。当对向量进行归一化后，欧式距离与余弦距离一致。

### 基本方法

#### 如何划分训练集？如何选取验证集？

> - 通常80%为训练集，20%为测试集
> - 当数据量较小时的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1
> - 当数据量很小时，可以采用K折交叉验证
> - 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）

#### 什么是偏差和方差？

> 偏差：描述预测值的期望与真实值之间的差别，偏差越大说明模型的预测结果越差。
>
> 方差：描述预测值的变化范围。方差越大说明模型的预测越不稳定。
>
> 高方差过拟合，高偏差欠拟合。

#### 什么是过拟合？深度学习解决过拟合的方法

> 过拟合是指模型拟合了训练样本中的噪声，导致泛化能力差。
>
> - 增加训练数据，增加数据多样性
> - 缩减模型表达能力，L1,L2正则化
> - Dropout
>- 训练时提前终止
> - 集成多种模型
>- BN
> 

#### 解决欠拟合的方法有哪些？

> 增加模型复杂度、调整模型初始化方式、调整学习率、集成多种模型

#### 深度模型参数调整的一般方法论

> 重要性：学习率>正则值>dropout
>
> - 学习率：遵循小->大->小原则
> - 初始化：选择合适的初始化方式，有预训练模型更好
> - 优化器选择：adam比较快，sgd较慢
> - loss：回归问题选L2 loss，分类问题选交叉熵
> - 可视化
> - 从小数据大模型入手，先过拟合，再增加数据并根据需要调整模型复杂度

### 优化方法

#### 简述了解的优化器，发展综述？

> $SGD \rightarrow Momentum\rightarrow NAG \rightarrow AdaGrad \rightarrow RMSProp \rightarrow Adam$ 
>
> $SGD : \theta = \theta-\alpha \nabla_{\theta}L$
>
> $Momentum: \theta = \theta+v \quad s.t.\ v=\beta v+\alpha \nabla_{\theta}L$ 定义了速度，更新不光取决于当前梯度，且保持了惯性。
>
> $AdaGrad (adaptive \ gradient):\theta=\theta-\alpha \bigodot g \quad s.t. \ \alpha = \frac{\alpha}{\delta+\sqrt{r}} \quad r = r+g \bigodot g$  通过过往的梯度累计，自动调节学习率。其中$g$为之前更新的参数梯度列表。
>
> $RMSProp: r = pr+(1-p)g \bigodot g$  累计梯度加权平均。
>
> $Adam:\theta = \theta-\alpha \frac{v}{\sqrt{r}+\epsilon} \quad s.t. \ r = pr+(1-p)g \bigodot g  \quad v=(1-\alpha)v+\alpha g$    
>
> Adam结合了动量，自适应学习率这两个特性，而这两个特性也叫一阶和二阶矩估计。
>















#### 常用的损失函数有哪些？分别适用于什么场景？

> - 分类损失：
>   - Log Loss
>   - Focal Loss
>   - KL divergence(Relative Entropy)
>   - Exponential Loss
>   - Hinge Loss
>
> - 回归损失：
>   - MSE(Mean Square Error)
>   - MAE(Mean Absolute Error)
>   - Huber Loss







#### 梯度下降与拟牛顿法的异同？

> 梯度下降和牛顿法的推导均与泰勒公式有关：
>
> 泰勒公式：$f(x)=\sum^{\infty}_{n=0} \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$ 
>
> ​				若 $x^t=x^{t-1}+\Delta x$ ，则$f(x^t)$ 在 $x^{t-1}$ 处的泰勒展开为：
>
> ​				$f(x^t)=f(x^{t-1}+\Delta x) \simeq f(x^{t-1})+f'(x^{t-1})\Delta x+f''(x^{t-1})\frac{{\Delta x}^2}{2}$
>
> 
>
> 1. 参数更新模式相同
>
> 2. 梯度下降法利用误差的梯度来更新参数，拟牛顿法利用海塞矩阵的近似来更新参数
>
> 3. 梯度下降是泰勒级数的一阶展开，而拟牛顿法是泰勒级数的二阶展开
>
> 4. SGD能保证收敛（神经网络中），但是L-BFGS在非凸时不收敛。
>
> 5. 梯度下降法收敛慢，迭代次数多，牛顿法收敛快，迭代次数少
>
> 6. 梯度下降可以用在特征维度较大的场景，牛顿法用在特征较少。

#### L1和L2正则分别有什么特点？为何L1稀疏？

> $P-Norm: ||x||_p:=(\sum^n_{i=1}|x_i|^p)^{\frac{1}{p}}$
>
> $L1$ 对异常点更具有鲁棒性、在0处不可导，计算不方便，没有唯一解，输出稀疏
>
> $L2$ 计算方便，对异常点敏感，有唯一解。
>
> 在梯度更新时，不管 $L1 $的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。从而导致$L1$输出稀疏

### 深度学习基础

#### 以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播

#### NN的权重参数能否初始化为0？

> 不能，可能导致模型无法收敛

#### 什么是梯度消失和梯度爆炸？

> - 梯度消失：
>   - 如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么在经过足够多层传播之后，误差对输入层的偏导会趋于0。这种情况会导致靠近输入层的隐含层神经元调整极小。
>   - ReLU、BN
>   - 残差网络结构
> - 梯度爆炸：
>   - 如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。这种情况又会导致靠近输入层的隐含层神经元调整变动极大。
>   - 梯度裁剪：设置一个梯度剪切阈值，更新梯度时，若梯度超过阈值，就将其强制限制在这个范围之内
>   - 正则化：通过正则化可以限制权值范数，通过正则化项，可以部分限制梯度爆炸的发生。
>   - ReLU、BN

#### 常用的激活函数，导数？

> 激活函数的条件：非线性、处处可微等。激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。
>
> - Sigmoid：$\sigma(x)=\frac{1}{1+e^{-x}} \quad \sigma'(x)=\sigma(x)(1-\sigma(x))$
> - Tanh：$tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \quad tanh'(x)=1-tanh^2(x)$
> - ReLU：$y = max(0, x) \quad y'=\begin{cases} 0, x<0\\ 1, x \ge 0 \end{cases}$
> - Leak-ReLU：$y =\begin{cases} \alpha x, x<0\\ x, x \ge 0 \end{cases} \quad y'=\begin{cases} \alpha, x<0\\ 1, x \ge 0 \end{cases}$
> - SoftPlus：$y=log(1+e^x) \quad y'=\sigma(x)$

#### relu的有优点？又有什么局限性？他们的系列改进方法是啥？

> Relu的主要贡献在于：解决了梯度消失、爆炸的问题、计算方便，计算速度快、加速了网络的训练、在负半区的导数为0，造成了网络的稀疏性，**缓解过拟合**
>
> 同时也存在一些缺点：
>
> 1. 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
> 2. 输出不是以0为中心的
>
> Leak-ReLU解决了relu的0区间带来的影响，而且包含了relu的所有优点。

#### sigmoid和tanh为什么会导致梯度消失？

> 当神经元的激活在接近0或1处时会饱和，在这些区域，梯度几乎为0。在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。如果局部梯度非常小，那么相乘的结果也会接近零，从而导致梯度消失，权重无法更新。为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。



- 一个隐层需要多少节点能实现包含n元输入的任意布尔函数？
- 多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？



#### dropout为何能防止过拟合？

> - 首先随机（临时）删掉网络中一定数量的隐藏神经元，输入输出神经元保持不变
> - 把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。按照随机梯度下降法更新（没有被删除的神经元）对应的参数（w，b）。
> - 恢复被删掉的神经元（被删除的神经元保持原样，被删除的神经元已经有所更新）重复这一过程。
>
> - 取平均的作用： 假设用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout不同的隐藏神经元就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均，可以达到整体上减少过拟合。
> - 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。
>



- dropout和BN在前向传播和方向传播阶段的区别？
- 交叉熵
- LR

### BN的优点

> 1. 子网络的输入分布会随着前段网络参数变化而改变（ICS问题），这会导致该子网络在每次参数更新够需要去适应新的输入分布，会难以训练。batchnorm保证的子网络的输入分布稳定
> 2. 在激活层前加batch norm有利于使输入处于激活函数的非饱和区，这样就允许使用tanh和sigmoid等饱和区大的激活函数了（之前激活函数饱和带来的梯度消失问题是通过把激活函数改成ReLU以及精细的参数初始化来解决的）
> 3. 特征的梯度尺度不再依赖于参数的初始化，可以允许使用更大的学习率，加快了网络收敛
> 4. 一定程度上是一种正则化，样本x随着batch1进行训练时，经过batchnorm的均值、方差为u1、v1，随着batch2进行训练时，经过batchnorm的均值、方差为u2，v2，可以防止网络对单个样本x的过拟合



### CNN

- 给定卷积核的尺寸，特征图大小计算方法？
- 网络容量计算方法
- 共享参数有什么优点
- 常用的池化操作有哪些？有什么特点？
- CNN如何用于文本分类？
- resnet提出的背景和核心理论是？
- 空洞卷积是什么？有什么应用场景？

### RNN

- 简述RNN，LSTM，GRU的区别和联系
- 画出lstm的结构图，写出公式
- RNN的梯度消失问题？如何解决？
- lstm中是否可以用relu作为激活函数？
- lstm各个门分别使用什么激活函数？
- 简述seq2seq模型？
- seq2seq在解码时候有哪些方法？
- Attention机制是什么？



## 机器学习

### 基础

- 样本不均衡如何处理？
- 什么是生成模型什么是判别模型？

### 集成学习

- 集成学习的分类？有什么代表性的模型和方法？
- 如何从偏差和方差的角度解释bagging和boosting的原理？
- GBDT的原理？和Xgboost的区别联系？
- adaboost和gbdt的区别联系？

### 模型 

- 手推LR、Kmeans、SVM
- SVM什么时候用线性核什么时候用高斯核
  > 1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
  >
  > 2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
  >
  > 3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况
  >
  > （1）如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
  >
  > （2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
  >
  > （3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。

- 简述ridge和lasson的区别和联系
- 树模型如何调参
- 树模型如何剪枝？
- 是否存一定存在参数，使得SVM的训练误差能到0
- 逻辑回归如何处理多分类？
- 决策树有哪些划分指标？区别与联系？
- 简述SVD和PCA的区别和联系？
- 如何使用梯度下降方法进行矩阵分解？
- LDA与PCA的区别与联系？

### 特征工程 

- 常用的特征筛选方法有哪些？
- 文本如何构造特征？
- 类别变量如何构造特征？
- 连续值变量如何构造特征？
- 哪些模型需要对特征进行归一化？
- 什么是组合特征？如何处理高维组合特征？

### 其他（分方向）

- word2vec的原理，glove的原理，fasttext的原理？

- cbow和skipgram如何选择？

- 了解elmo和bert吗？简述与word embedding的联系和区别

- 图像和文本和语音数据各有哪些数据增强方法？

- rcnn、fatse rcnn、fatser rcnn、mask rcnn的原理？ 

- 介绍resnet和GoogLeNet中的inception module的结构？ 

- 介绍yolo和ssd ？

- 介绍FM，FFM，deepFM，deepWide.

- 机器翻译如何解决oov？

- 等等

  

  

## 数据结构与算法

## 二叉树类

- 前中后的非递归？
- 层次遍历，之字遍历？
- 二叉树的序列化与反序列化
- 前中，后中遍历结果恢复二叉树
- 排序二叉树的序列化
- 二叉树的直径
- 二叉树的路径和为定值的路径
- 翻转、复制二叉树
- 排序二叉树转双向链表
- 判断二叉树是否相同？判断一棵树是不是另一棵树的子树？

### 搜索回溯

- 八皇后，全排列，组合
- 重复数字的排列，重复数字的组合
- 图的搜索
- A star

### 概率题 

- 用rand7构造rand10
- 轮盘赌
- 三角形内等概论随机选点
- 等等

### 动态规划

- 编辑距离
- 背包
- LCS

## 字符串

- 给定字符串是否符合正则表达式XXX
- 给定字符串是否是数字？
- KMP
- 超大数相加

## 数组、二分、快排系列

- 旋转数组查找
- 数组中的topk
- 旋转打印数组
- 行列递增矩阵查找数字
- 查找出现超过一半的数字
- 查找和为定值的两个数

### 链表

- 翻转链表
  
  > 这个没啥问题吧
- 两个链表是否有交点
  
  > 把一个链表中的所有节点地址存到一个哈希表中，遍历另一个链表，比对
- 一个链表是否有环
  
  > 用哈希表存每个节点的地址，遍历链表，如果该节点在哈希表中则有环
- 链表的倒数k个节点
  
  > 指针p1指向链表头，p2指向第k个节点，两个指针同时后移，当p2指向链表尾（p2->next==NULL）时，p1指向倒数第k个节点
- 合并链表
  
  > 这个也没啥吧
- 逆序打印链表
  
  > 顺序入栈，然后出栈，打印

### 排序

- 各种排序的复杂度最优最差平均
  
  > https://www.cnblogs.com/love-jelly-pig/p/8450459.html
- 堆排序topk
  
  > 维护一个长度为k的小顶堆（用初始化序列的前10个数字初始化），从k+1个数字开始遍历，如果大于堆顶，则堆顶出堆，该数字入堆
- 快排的变化
  > 随机选取基准
  > 三数取中
  > 当序列被分割到一定大小后采用插入排序
  > 尾递归


## 海量数据题

- 海量日志的出现最多的10个字符串
  
  > 即上述topk
- 10亿个1-10的数字排序
  
  > 如果只有1-10可以直接统计计数，O(n)。
  > 如果不只有1-10，拆分成多个文件，每个文件分别排序，再多路归并写入一个文件，最后扫一遍检查一下
- trie树
- hashmap
- 布隆过滤器
- bitmap













字节面试

### 交叉熵公式，已知p和y，写代码计算交叉熵

> 二分类交叉熵公式：$L = -\frac{1}{n}\sum_i^n[(1-p)log(1-y)+plogy]$
>
> 多分类交叉熵公式：$L=-\frac{1}{n}\sum_i^n\ \sum^m_j p_j^ilog\ y^i_j$ ，其中 $i$ 为样本id，$j$ 为类别id

### 为什么最小化交叉熵可以实现分类

> - **信息熵是消除不确定性所需信息量的度量。**信息熵就是信息的不确定程度，信息熵越小，信息越确定。$信息熵=∑^n_{x=1}(信息x发生的概率×验证信息x需要的信息量) \rightarrow\ H(X)=-\sum_xp(x)log(p(x))$ 为事件发生的可能性。
>
> - 相对熵：对于随机变量 $x$ 有两个独立的概率分布 $p(x)$ 和 $q(x)$。相对熵为衡量两个分布的差异，也称为KL散度：$D_{KL}(p||q) = \sum_{i=1}^np(x_i)log\frac{p(x_i)}{q(x_i)}$  越小，$D_{KL}(p||q)$ 越小，表示p(x)和q(x)的分布越近。
>
> 交叉熵：$H(p, q)=-\sum_{i=1}^np(x_i)log(q(x_i))$ 
>
> 根据相对熵推导 $D_{KL}(p||q)= \sum_{i=1}^np(x_i)log(p(x_i))- \sum_{i=1}^np(x_i)log(q(x_i)) =  H(p, q)-H(X)$
>
> 在最小化交叉熵过程中，设定分布 $p(x)$ 为真实分布，而 $q(x)$ 为模型的预测分布。优化两个分布的距离可以从优化交叉熵中实现。并且交叉熵形式更简洁，计算方便。

### bn怎么计算，其他Norm方式相比bn的特点和优势

> <img src="img/BN公式.png" width=400px>
>
> 对于BN，mean计算的是每一个长宽和batch的平均值，若输入bs为16的图片，维度为$5\times5\times128$ 的特征，mean的维度为128维。
>
> 不同的Norm方式：
>
> <img src="img/Norm.png" width=600px>
>
> - Batch Normalization：
>   - BN的计算就是把每个通道的NHW单独拿出来归一化处理
>   - 针对每个channel我们都有一组γ,β，所以可学习的参数为2*C
>   - 当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局
>
> - Layer Normalizaiton：
>   - LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响
>   - 常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理
>
> - Instance Normalization
>   - IN的计算就是把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响
>   - 常用在风格化迁移，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理
>
> - Group Normalizatio
>   - GN的计算就是把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW
>   - GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C
>
> - Switchable Normalization：将 BN、LN、IN 结合，赋予权重，让网络自己去学习归一化层应该使用什么方法，训练复杂


### LR和softmax+cross entropy有什么区别

> softmax公式：$softmax(x_i)=\frac{e^{x_i}}{\sum^n_je^{x_j}}, \quad j=1,2,..., n$ 
>
> LR采用的是sigmoid公式：$\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{e^x+1}$ 
>
> 从公式上可以看出，LR为softmax+cross entropy 的特殊方式，二分类情况下，LR可以看成将一类输出固定为0，只需要优化另外一类的输出即可。

### Softmax的数值稳定性怎么解决

> 当数值过小的时候，被四舍五入为0，这就是下溢出。做除法运算等会出问题。反之，当数值过大的时候，情况就变成了上溢出。
>
> softmax公式：$softmax(x_i)=\frac{e^{x_i}}{\sum^n_je^{x_j}}, \quad j=1,2,..., n$ 
>
> softmax数值不稳定问题：假设$n=3$，所有$x_i=c$，则计算得到的概率均为$\frac{1}{3}$，但是如果 $c$ 极大，则出现上溢出，若 $c$ 极小为负数，则分母可能四舍五入为0导致下溢出。
>
> 解决数值稳定性：
>
> - 减去最大值解决：
>   - 找到 $x_i$ 中的最大值$m=max(x_i)$，将 $softmax(x_i)$ 改为 $softmax(x_i-m)$ 可以解决问题，同时计算结果不变。
>   - 通过这样的变换，对任何一个 $x_i$，减去 $m$ 之后，$e$ 的指数的最大值为0，所以不会发生上溢出；同时，分母中也至少会包含一个值为1的项，所以分母也不会下溢出。
>   - 计算结果不变：$softmax(x_i-m)=\frac{e^{x_i-m}}{\sum^n_je^{x_j-m}}=\frac{\frac{e^{x_i}}{e^m}}{\sum^n_j\frac{e^{x_j}}{e^m}}=\frac{e^{x_i}}{\sum^n_je^{x_j}}$
>
> - log softmax 解决：
>
>   - $$
>     log\ softmax(x_i)=log(\frac{e^{x_i}}{\sum^n_je^{x_j}})=log(\frac{e^{x_i-m}}{\sum^n_je^{x_j-m}})=log(e^{x_i-m})-log(\sum^n_je^{x_j-m})=(x_i-m)-log(\sum^n_je^{x_j-m})
>     $$
>
>   - 最后的表达式中，会产生下溢出的因素已经被消除掉了——求和项中，至少有一项的值为1，这使得log后面的值不会下溢出，也就不会发生计算 log(0) 的情况。

