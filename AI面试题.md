# 算法工程师面试

## 深度学习 

### 模型评估方法

#### Accuracy作为指标有哪些局限性？

> 当正负样本极度不均衡时存在问题！当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。

#### ROC曲线和PR曲线各是什么？

> 受试者工作特征曲线（Receiver Operating Characteristic，ROC）
> ROC关注两个指标：
> 1） True Positive Rate ( TPR ) = TP / [ TP + FN] ，TPR代表能将正例分对的概率
> 2）False Positive Rate( FPR ) = FP / [ FP + TN] ，FPR代表将负例错分为正例的概率
>
> PR（Precision - Recall）曲线 
> 准确率（Precision，P）：P = TP / (TP + FP)
> 召回率（Recall，R）：R = TP/ (TP + FN)
> **PR曲线实则是以precision（精准率）和recall（召回率）这两个为变量而做出的曲线，其中recall为横坐标，precision为纵坐标。**
> 平衡点（BEP）是P=R时的取值，如果这个值较大，则说明学习器的性能较好。而F1 = 2 * P * R ／( P + R )，同样，F1值越大，我们可以认为该学习器的性能较好。
> ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。
>
> True Positive, TP: 预测为正样本，实际也为正样本
> False Positive, FP: 预测为正样本，实际为负样本
> True Negative, TN: 预测为负样本，实际也为负样本
> False Negative, FN: 预测为负样本，实际为正样本

#### 编程实现AUC的计算，并指出复杂度？

> 有两种计算AUC的方法:
> 1.绘制ROC曲线，ROC曲线下面的面积就是AUC的值
> 2.假设总共有（m+n）个样本，其中正样本m个，负样本n个，总共有mn个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以（mn）就是AUC的值
> 
> ```python
> def calc_auc(y_labels, y_scores):
>  f = list(zip(y_scores, y_labels))
>  rank = [values2 for values1, values2 in sorted(f, key=lambda x:x[0])]
>  rankList = [i+1 for i in range(len(rank)) if rank[i] == 1]
>  pos_cnt = np.sum(y_labels == 1)
>  neg_cnt = np.sum(y_labels == 0)
>  auc = (np.sum(rankList) - pos_cnt*(pos_cnt+1)/2) / (pos_cnt*neg_cnt)
>  print(auc)
> ```
>
> 排序复杂度：$O(log2(P+N))$
>
> 计算AUC的复杂度：$O(P+N)$

#### AUC指标有什么特点？放缩结果对AUC是否有影响？

> **AUC (Area under Curve)：ROC曲线下的面积，作为数值可以直观的评价分类器的好坏，值越大越好。**
> $ AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
>  $0.5 < AUC < 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
>  $AUC = 0.5$，模型没有预测价值
>
> 放缩结果对$AUC$没有影响

#### 余弦距离与欧式距离有什么特点？

> 余弦距离$cos\theta=\frac{<x,y>}{||x||||y||}$
>
> 欧氏距离： $d(x,y)=\sqrt{\sum_{i=0}^N(x_i-y_i)^2}$
>
> 余弦距离，也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡畺两个个体间差异的大小的度量。如果两个向量的方向一致，即夹角接近零，那么这两个向量就相近。
>
> 余弦距离使用两个向量夹角的余弦值作为衡是两个个体间差异的大小。
>
> 相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。当对向量进行归一化后，欧式距离与余弦距离一致。

### 基本方法

#### 如何划分训练集？如何选取验证集？

> - 通常80%为训练集，20%为测试集
> - 当数据量较小时的时候将训练集、验证集以及测试集划分为6：2：2；若是数据很大，可以将训练集、验证集、测试集比例调整为98：1：1
> - 当数据量很小时，可以采用K折交叉验证
> - 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）

#### 什么是偏差和方差？

> 偏差：描述预测值的期望与真实值之间的差别，偏差越大说明模型的预测结果越差。
>
> 方差：描述预测值的变化范围。方差越大说明模型的预测越不稳定。
>
> 高方差过拟合，高偏差欠拟合。

#### 什么是过拟合？深度学习解决过拟合的方法

> 过拟合是指模型拟合了训练样本中的噪声，导致泛化能力差。
>
> - 增加训练数据，增加数据多样性
> - 缩减模型表达能力，L1,L2正则化
> - Dropout
>- 训练时提前终止
> - 集成多种模型
>- BN
> 

#### 解决欠拟合的方法有哪些？

> 增加模型复杂度、调整模型初始化方式、调整学习率、集成多种模型

#### 深度模型参数调整的一般方法论

> 重要性：学习率>正则值>dropout
>
> - 学习率：遵循小->大->小原则
> - 初始化：选择合适的初始化方式，有预训练模型更好
> - 优化器选择：adam比较快，sgd较慢
> - loss：回归问题选L2 loss，分类问题选交叉熵
> - 可视化
> - 从小数据大模型入手，先过拟合，再增加数据并根据需要调整模型复杂度

### 优化方法

#### 简述了解的优化器，发展综述？

> $SGD \rightarrow Momentum\rightarrow NAG \rightarrow AdaGrad \rightarrow RMSProp \rightarrow Adam$ 
>
> $SGD : \theta = \theta-\alpha \nabla_{\theta}L$
>
> $Momentum: \theta = \theta+v \quad s.t.\ v=\beta v+\alpha \nabla_{\theta}L$ 定义了速度，更新不光取决于当前梯度，且保持了惯性。
>
> $AdaGrad (adaptive \ gradient):\theta=\theta-\alpha \bigodot g \quad s.t. \ \alpha = \frac{\alpha}{\delta+\sqrt{r}} \quad r = r+g \bigodot g$  通过过往的梯度累计，自动调节学习率。其中$g$为之前更新的参数梯度列表。
>
> $RMSProp: r = pr+(1-p)g \bigodot g$  累计梯度加权平均。
>
> $Adam:\theta = \theta-\alpha \frac{v}{\sqrt{r}+\epsilon} \quad s.t. \ r = pr+(1-p)g \bigodot g  \quad v=(1-\alpha)v+\alpha g$    
>
> Adam结合了动量，自适应学习率这两个特性，而这两个特性也叫一阶和二阶矩估计。
>
> 

#### 常用的损失函数有哪些？分别适用于什么场景？

> - 分类损失：
>   - Log Loss：
>   - cross-entropy loss：$L(Y|f(X))=$
>   - Focal Loss
>   - KL divergence(Relative Entropy)
>   - Exponential Loss：$L(Y|f(X))=exp(-ylogf(x))$ 对离群点和噪声敏感
> - Hinge Loss：$L(Y|f(X))=max(0, 1-yf(x))$ 0点不可导，不能用梯度下降。
>   
> - 回归损失：
>   - MSE(Mean Square Error)：$L(Y|f(X))=\sum_N(y-f(x))^2$ 当预测值距离真是只越远时，平方损失函数的惩罚力度越大，因此对异常点比较敏感。
>   - MAE(Mean Absolute Error)：$L(Y|f(X))=\sum_N|f(x)-y|$  绝对损失函数相当于在做中值回归，相比做均值回归的平方损失函数，绝对损失函数对异常点更鲁棒。但在f=y处无法求导。
>   - Huber Loss： $L(Y|f(X))=\begin {cases} \sum_N(f(x)-y)^2, |f(x)-y|< \delta \\ \sum_N|f(x)-y|, |f(x)-y|>\delta \end{cases}$Huber损失函数在|f-y|较小时为平方损失，在|f-y|较大的时采用线性损失，处处可导，且对异常点鲁棒。

#### 交叉熵、相对熵、log损失

> cross-entropy loss： $L(Y|f(X))=\sum_Nylogf(x)+(1-y)log(1-f(x))$ 
>
> relative-entropy loss：
>
> log loss：



#### 梯度下降与拟牛顿法的异同？

> 1.参数更新模式相同
>
> 2.梯度下降法利用误差的梯度来更新参数，拟牛顿法利用海塞矩阵的近似来更新参数
>
> 3.梯度下降是泰勒级数的一阶展开，而拟牛顿法是泰勒级数的二阶展开
>
> 4.SGD能保证收敛，但是L-BFGS在非凸时不收敛

#### L1和L2正则分别有什么特点？为何L1稀疏？

> $P-Norm: ||x||_p:=(\sum^n_{i=1}|x_i|^p)^{\frac{1}{p}}$
>
> $L1$ 对异常点更具有鲁棒性、在0处不可导，计算不方便，没有唯一解，输出稀疏
>
> $L2$ 计算方便，对异常点敏感，有唯一解。
>
> 在梯度更新时，不管 $L1 $的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。从而导致$L1$输出稀疏

### 深度学习基础

#### 以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播

#### NN的权重参数能否初始化为0？

> 不能，可能导致模型无法收敛

#### 什么是梯度消失和梯度爆炸？

> - 梯度消失：
>   - 如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么在经过足够多层传播之后，误差对输入层的偏导会趋于0。这种情况会导致靠近输入层的隐含层神经元调整极小。
>   - ReLU、BN
>   - 残差网络结构
> - 梯度爆炸：
>   - 如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。这种情况又会导致靠近输入层的隐含层神经元调整变动极大。
>   - 梯度裁剪：设置一个梯度剪切阈值，更新梯度时，若梯度超过阈值，就将其强制限制在这个范围之内
>   - 正则化：通过正则化可以限制权值范数，通过正则化项，可以部分限制梯度爆炸的发生。
>   - ReLU、BN

#### 常用的激活函数，导数？

> 激活函数的条件：非线性、处处可微等。激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。
>
> - Sigmoid：$\sigma(x)=\frac{1}{1+e^{-x}} \quad \sigma'(x)=\sigma(x)(1-\sigma(x))$
> - Tanh：$tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \quad tanh'(x)=1-tanh^2(x)$
> - ReLU：$y = max(0, x) \quad y'=\begin{cases} 0, x<0\\ 1, x \ge 0 \end{cases}$
> - Leak-ReLU：$y =\begin{cases} \alpha x, x<0\\ x, x \ge 0 \end{cases} \quad y'=\begin{cases} \alpha, x<0\\ 1, x \ge 0 \end{cases}$
> - SoftPlus：$y=log(1+e^x) \quad y'=\sigma(x)$

#### relu的有优点？又有什么局限性？他们的系列改进方法是啥？

> Relu的主要贡献在于：解决了梯度消失、爆炸的问题、计算方便，计算速度快、加速了网络的训练、在负半区的导数为0，造成了网络的稀疏性，**缓解过拟合**
>
> 同时也存在一些缺点：
>
> 1. 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
> 2. 输出不是以0为中心的
>
> Leak-ReLU解决了relu的0区间带来的影响，而且包含了relu的所有优点。

#### sigmoid和tanh为什么会导致梯度消失？

> 当神经元的激活在接近0或1处时会饱和，在这些区域，梯度几乎为0。在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。如果局部梯度非常小，那么相乘的结果也会接近零，从而导致梯度消失，权重无法更新。为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。

#### 一个隐层需要多少节点能实现包含n元输入的任意布尔函数？

> 

#### 多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？

> 
>

#### dropout为何能防止过拟合？

> - 首先随机（临时）删掉网络中一定数量的隐藏神经元，输入输出神经元保持不变
> - 把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。按照随机梯度下降法更新（没有被删除的神经元）对应的参数（w，b）。
> - 恢复被删掉的神经元（被删除的神经元保持原样，被删除的神经元已经有所更新）重复这一过程。
>
> - 取平均的作用： 假设用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout不同的隐藏神经元就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均，可以达到整体上减少过拟合。
> - 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。
>
> dropout是在全连接层——延伸dropblock是在卷积层，是卷积网络的专属正则化（dropout在conv上效果不好的原因：conv具有空间相关性，做drop会有信息流向后面的网络，导致dropout不彻底。）

#### dropout和BN 在前向传播和方向传播阶段的区别

> **Batch Normalization**，就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。
>
> - 在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
>   - 求解输入向量每一维度特征的均值和方差；
>   - 对每一维度特征就行0均值，1方差进行归一化；
>   - 引入可学习参数**γ**和**β**，对归一化后的结果进行缩放(scale)和平移(shift)操作。
> - 在测试时，用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。
>   - $E(x) \leftarrow E_{B}(\mu_B) \quad Var(x) \leftarrow \frac{m}{m-1} E_B(\sigma^2_B)$
>   - $y=\gamma (\frac{x-E(x)}{\sqrt{Var(x)+\epsilon}})+\beta$
>
> **但是**，实际求解均值E[x]和方差Var[x]计算量较大，在实际软件框架实现中，采用滑动平均法(Moving Average)来近似求解E[x]和Var[x]。即在训练阶段，每更新一次参数，都顺便按照滑动平均法更新均值和方差，最终训练过程结束后得到的均值和方差就用来代替这里的E[x]和方差Var[x]。
>
> 滑动平均按照如下公式计算：shadow_variable（cur） = decay * shadow_variable（pre） + (1 - decay) * variable，decay是人为设定的衰减率参数，shadow_variable（cur）相当于当前轮次训练的参数更新，它等于上一轮次的参数更新*衰减率+当前轮次计算出的参数取值*（1-衰减率）。这里所说的参数就是均值和方差，当前计算出的参数取值就是当前伦次训练过程中计算出的batch的均值和方差。（具体什么是滑动平均，自行百度）
>
> 当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。
>
> BN训练**不用全量训练集的均值和方差**：因为用全量训练集的均值和方差容易过拟合，对于BN，其实就是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上能够增加模型的鲁棒性，也会在一定程度上减少过拟合。BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。
>
> **Dropout**
> Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。
>
> Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。在测试时，应该用整个训练好的模型，因此不需要dropout。
>

- LR

#### 初始化方式





### CNN

#### 给定卷积核的尺寸，特征图大小计算方法？

> $size = \frac{input-ksize+2\times padding}{strides}+1$

#### 网络容量计算方法

> 卷积：$ksize \times ksize \times in\_channel \times out\_channel +out\_channel(bias)$
>
> 池化：无参数
>
> 全连接：$feature\_W\times feature\_H \times in\_channel \times out\_channel +1 $

#### 共享参数有什么优点

> 1.削减参数量，压缩模型复杂度
>
> 2.实现平移不变性

#### 常用的池化操作有哪些？有什么特点？

> 抑制噪声，降低信息冗余
> 提升模型的尺度不变性、旋转不变形
> 降低模型计算量
> 防止过拟合
>
> - 最大池化、平均池化：最大池化又分为重叠池化和非重叠池化，比如常见的stride=kernel size的情况属于非重叠池化，如果stride<kernel size 则属于重叠池化。重叠池化相比于非重叠池化不仅可以提升预测精度，同时在一定程度上可以缓解过拟合。
> - 随机池化：特征区域的大小越大，代表其被选择的概率越高
> - 中值池化：用的非常少，中值池化也具有学习边缘和纹理结构的特性，抗噪声能力比较强。
> - 组合池化：组合池化则是同时利用最大值池化与均值池化两种的优势而引申的一种池化策略。其作用就是丰富特征层，maxpool更关注重要的局部特征，而average pooling更关注全局特征。
> -  双线性池化：双线性池化主要用于特征融合，对于同一个样本提取得到的特征x和特征y, 通过双线性池化来融合两个特征(外积)，进而提高模型分类的能力。

#### CNN如何用于文本分类？



#### resnet提出的背景和核心理论是？

> 当模型层数增加到某种程度，模型的效果将会不升反降。深度模型发生了退化（degradation）情况。
>
> 这是因为梯度爆炸/消失（弥散）。由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的（信息损失）。Residual Learning的初衷，其实是让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化。
>
> 但如果把网络设计为**H(x) = F(x) + x，即直接把恒等映射作为网络的一部分**。就可以把问题转化为**学习一个残差函数F(x) = H(x) - x.**
>
> 只要F(x)=0，就构成了一个恒等映射H(x) = x。 而且，拟合残差至少比拟合恒等映射容易得多。残差的思想都是去掉相同的主体部分，从而突出微小的变化，

- 空洞卷积是什么？有什么应用场景？

### RNN

- 简述RNN，LSTM，GRU的区别和联系
- 画出lstm的结构图，写出公式
- RNN的梯度消失问题？如何解决？
- lstm中是否可以用relu作为激活函数？
- lstm各个门分别使用什么激活函数？
- 简述seq2seq模型？
- seq2seq在解码时候有哪些方法？





#### Attention机制是什么？



## 机器学习

### 基础

#### 样本不均衡如何处理？

> - 选择合适的**评价指标**：选用F1、ROC等，不要用accuracy
> - 极度不均衡采用**异常检测**算法
> - **欠采样和过采样**
>   - 缺点：
>     - 过采样：过拟合风险；
>     - 欠采样：样本缺失，偏差较大；
> - **加权Loss：**
>   对待不均衡样本，可以对不同类别进行加权。比如样本比较少的类别，可以给予较高的权重。
> - **设计使用于不平衡数据集的模型**：XGBoost、调整SVM以惩罚稀有类别的错误分类

#### 什么是生成模型什么是判别模型？

> 判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。
>
> 生成模型：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。常见的有NB HMM模型。

### 集成学习

- 集成学习的分类？有什么代表性的模型和方法？
- 如何从偏差和方差的角度解释bagging和boosting的原理？
- GBDT的原理？和Xgboost的区别联系？
- adaboost和gbdt的区别联系？

### 模型 

- 手推Kmeans、SVM
- SVM什么时候用线性核什么时候用高斯核

> 

- 简述ridge和lasson的区别和联系
- 树模型如何调参
- 树模型如何剪枝？
- 是否存一定存在参数，使得SVM的训练误差能到0
- 逻辑回归如何处理多分类？
- 决策树有哪些划分指标？区别与联系？
- 简述SVD和PCA的区别和联系？
- 如何使用梯度下降方法进行矩阵分解？
- LDA与PCA的区别与联系？

### 特征工程 

- 常用的特征筛选方法有哪些？
- 文本如何构造特征？
- 类别变量如何构造特征？
- 连续值变量如何构造特征？
- 哪些模型需要对特征进行归一化？
- 什么是组合特征？如何处理高维组合特征？

### 其他（分方向）

- word2vec的原理，glove的原理，fasttext的原理？

- cbow和skipgram如何选择？

- 了解elmo和bert吗？简述与word embedding的联系和区别

- 图像和文本和语音数据各有哪些数据增强方法？

- rcnn、fatse rcnn、fatser rcnn、mask rcnn的原理？ 

- 介绍resnet和GoogLeNet中的inception module的结构？ 

- 介绍yolo和ssd ？

- 介绍FM，FFM，deepFM，deepWide.

- 机器翻译如何解决oov？

- 等等

  


## 模型工具

#### 模型压缩和加速

> - 轻量级模型：加速网络结构设计，通过优化网络结构的设计去减少模型的冗余和计算量。主要采用深度可分离卷积
>   - SequenceNet：简单的维度压缩和$1 \times 1$卷积堆叠
>   - MobileNet：采用深度可分离卷积对模型进行参数压缩，对于深度卷积中的信息不流畅问题采用point-wise Convolution。
>   - ShuffleNet：主要改进在分组卷积中，不同group之间无法进行信息共享，提出了channel shuffle的方法。
>   - Xception：假设网络中空间卷积和通道卷积分开做会比较好。
> - 模型裁剪和模型压缩：
>   - 





