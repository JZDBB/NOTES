# Attention

attention是一种能让模型对重要信息重点关注并充分学习吸收的技术

所以说Attention的作用是？
Attention的出现就是为了两个目的：1. 减小处理高维输入数据的计算负担，通过结构化的选取输入的子集，降低数据维度。2. “去伪存真”，让任务处理系统更专注于找到输入数据中显著的与当前输出相关的有用信息，从而提高输出的质量。Attention模型的最终目的是帮助类似编解码器这样的框架，更好的学到多种内容模态之间的相互关系，从而更好的表示这些信息，克服其无法解释从而很难设计的缺陷。从上述的研究问题可以发现，Attention机制非常适合于推理多种不同模态数据之间的相互映射关系，这种关系很难解释，很隐蔽也很复杂，这正是Attention的优势—不需要监督信号，对于上述这种认知先验极少的问题，显得极为有效。
