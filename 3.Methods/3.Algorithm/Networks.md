# Tips for Networks

### copy branch

类似于一些multi-task learning，在简单的神经网络的训练过程中，能将network中的一些层拓开，并作为一个新的网络输出，和主网络共享参数，相当于现有网络的分支copy，并在最后的输出中计算Loss。测试中将分支砍掉，只测试主网络。实验表明，改方法可以一定程度上的提升网络任务的性能。与此同时，该训练方法会加大训练中的计算量，测试应用过程中，和单独的主网络无差别。



### mode compress & speed up

**分组卷积**

分组卷积即将输入的feature maps分成不同的组（沿channel维度进行分组），然后对不同的组分别进行卷积操作，即每一个卷积核至于输入的feature maps的其中一组进行连接，而普通的卷积操作是与所有的feature maps进行连接计算。分组数k越多，卷积操作的总参数量和总计算量就越少（减少k倍）。然而分组卷积有一个致命的缺点就是不同分组的通道间减少了信息流通，即输出的feature maps只考虑了输入特征的部分信息，因此在实际应用的时候会在分组卷积之后进行信息融合操作，接下来主要讲两个比较经典的结构，ShuffleNet[1]和MobileNet[2]结构。1) ShuffleNet结构：

<img src="..\img\shuffle net.jpg">

如上图所示，图a是一般的group convolution的实现效果，其造成的问题是，输出通道只和输入的某些通道有关，导致全局信息 流通不畅，网络表达能力不足。图b就是shufflenet结构，即通过均匀排列，把group convolution后的feature map按通道进行均匀混合，这样就可以更好的获取全局信息了。 图c是操作后的等价效果图。在分组卷积的时候，每一个卷积核操作的通道数减少，所以可以大量减少计算量。