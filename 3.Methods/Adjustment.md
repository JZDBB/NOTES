# 调参tips

神经网络针对不同的问题有不同的解决方法： 

<img src=".\img\adjust.png" height="400px">

## 训练开始前

**1.使用适当的日志和有意义的变量名**。在每个训练步骤中，记录相关的值，比如：step_number、accuracy、loss、learning_rate，甚至有时候还包括一些更具体的值。

3.实施数据增强技术。虽然这一点并不是对所有情况都适用，不过如果你在搞图像相关的神经网络，用简单的数据增强技术处理一下图像，例如镜像、旋转、随机裁剪和重新缩放、添加噪声、弹性变形等，大部分时候出来的效果都有巨大提升。 

4.对所有层使用权重初始化和正则化。不要把权重初始化为相同的值，当然你要是把它们都初始化成0……那就更糟了，这可能会引入对称性，并且导致梯度消失，大多数时候都会导致糟糕的结果。一般情况下，如果你在权重初始化时遇到问题，你可以考虑在神经网络中添加批量标准化层（Batch Normalization Layer）。 

## 训练过程中

**1、使用Adam作为优化器。**Adam优化效果非常好。是首选。Tensorflow实践笔记：当保存和回复模型参数时，设置AdamOptimizer之后，一定记得设置Saver，因为Adam有些state也需要恢复（即每个weight的学习率）。 

    **2、采用ReLu作为非线性激活函数。**Relu训练速度非常快，简单，而且训练效果非常好，不存在梯度消失的问题。 

    **3、在网络的输出层不要使用激活函数。** 

    **4、在每一层都添加偏置项（bias）。**因为偏置项很重要，可以把一个平面转换成一个best-fitting position。比如，y=mx+b，b就是偏置项，它使得一条直线上线移动，以便找到最优的position。 

    **5、使用variance-scaled 初始化。**在Tensorflow中，就是这个接口：tf.contrib.layers.variance_scaling_initializer()。经验中，这种generalizes/scales比其他常用的初始化方法要好，如Gaussian，truncated normal和Xavier。 

    **6、Whiten（规范化，normalize）输入数据**。训练时，减去输入数据的均值，然后除以输入数据的方差。模型的权重延伸和拓展的角度越小，网络学习更容易且速度越快。保持输入数据是零均值的（mean-centered）且具有恒大的方差，可以帮助实现这一点。对于所有的测试数据也需要执行这一点，所以一定要确保你的训练数据与真实数据高度相似。 

    **7、在保留输入数据dynamic range的情况下，对输入数据进行尺度变换。**这个操作与normalization相关，但应先于normalization执行。举个例子，数据X实际的变化范围是[0, 140000000]，这可以被激活函数tanh（x）或tanh（x/c）所驯服（c是一个常量，延展曲线，在输入数据的变化范围匹配（fit）输入的动态特性，即tanh函数倾斜（激活）部分）。特别是你的输入数据根本没有一个上下限变化范围时，神经网络可以在（0，1）范围内，学得更好。 

    **8、不要影响learning rate的衰减。**学习速率的衰减在SGD中很常见，而且ADAM中也会自然调整它。如果你想直接一点点的调整学习速率，比如，在训练一段时间后，减小学习速率，误差曲线可能会突然drop一点点，随后很快恢复平整。 

    **9、如果你使用的卷积层使用64或128的卷积核，这已经足够了。**对于深度网络，128已经很大了，增加更多的卷积核，并不会带来效果的提升。 

    **10、Pooling（池化）以保持转移不变性。**Pooling主要是让网络对于图片输入的“该部分”具有一个“普适的作用”。比如说，最大池化（Max pooling）可以帮助CNN对于发生旋转、偏置和特征尺度变换的输入图像，也能够具有的鲁棒性。 

  

## **神经网络诊断tips** 

    如果网络不学习（意思是：loss/accuracy在训练过程中不收敛，或者没有得到你预期的结果），尝试下面的tips： 

    **1、过拟合。**如果你的模型不学习，首先应该想到模型是否陷入了过拟合情况。在一个很小的数据集上训练模型，准确率达到了100%或99.99%，或者error接近于0。如果你的神经网络不能实现过拟合，说明你的网络结构存在一些严重的问题，但也可能不是很明显。如果你的模型在小数据上过拟合，但在大数据集上任然不收敛，试一试下面的suggestion。 

    **2、减小学习速率。**模型的学习速度会变慢，但模型可以到达一个更小的局部极小值，这个点因为之前的步长过大而跳不进来。 

    **3、增大学习速率。**这可以加速模型的训练，尽快收敛，帮助模型跳出局部极小值。尽管神经网络很快就收敛了，但是其结果并不是最好的，其“收敛”得到的结果可能会及其不稳定，即不同训练，得到结果差别很大。（使用Adam，我们发现0.001是一个很好的初始值）。 

    **4、减小（mini）batch size。**减小batch size至1可以得到模型参数调整最细粒度的变化，你可以在Tensorboard（或其他debugging/可视化工具）中观察到，确定梯度更新是否存在问题。 

    **5、去除batch normalization。**当你把batch size减小至1时，起初BN可以帮助你发现模型是否存在梯度爆炸或梯度消失的问题。曾经，我们有一个神经网络不收敛，仅当我们去除BN之后我们才发现模型的输入在第二个iteration是变成了NaN。BN是一种锦上添花的措施，它只有在你确定你的模型不存在其他问题时才能正常发挥其强大的功能。 

    **6、增大（mini-）batch size。**增加batch size是必须的，越大的batch size可以减小梯度更新的方差，使得每一轮的梯度更新更加准确。换句话说，梯度更新会沿着准确的方向移动。但是！我们不可能无限制的增加batch size，因为计算机的物理内存是有限的。经验证明，这一点没有之前提出的两个suggestion重要，即减小batch size和去除BN。 

    **7、检查你的reshape操作。**频繁的进行reshape操作（比如，改变图像X和Y的维度）可能会破环空间的局部特性（spatially local features），使得神经网络几乎不能准确学习，因为它们必须学习错误的reshape。（自然的features变得破碎不堪，因为CNN的卷积操作高度依赖这些自然情况下的局部空间特征）同时对多个图片/channels进行reshape操作时必须非常的小心，使用numpy.stack()进行正确的对齐。 

    **8、监视你loss函数的变化。**如果使用的时一个复杂的函数作为目标函数，尽量使用L1或L2约束去去简化它。我们发现，L1对于边界没有那么敏感，当模型遇到噪声训练数据时，模型参数调整没有那么剧烈。 

    **9、尽可能的进行模型训练可视化。**如果你有可视化的工具，如matplotlib、OpenCV、Tensorboard等等，尽量可视化网络中值得scale、clipping等得变化，并保证不同参数着色策略得一致性。 











 



 

 

 

 

**损失没有改善怎么办？** 

如果，训练了好几个Epoch，损失还是没有变小，甚至还越来越大，就要： 

 

1.确认你用的损失函数是合适的，你优化的张量也是对的。

 

 

3.确认变量真的在训练。要检查这个，就得看张量板的直方图。 

 

或者写个脚本，在几个不同的训练实例 (training instances) 中，算出每个张量的范数。 

如果变量没在训练，请看下节，“变量没在训练怎么办？”。 

 

 

**如果有个变量不训练：** 

 

1.确认TF把它看做可训练 (trainable) 的变量。。 

 

2.确认没发生梯度消失。 



  

 

如果大部分神经元电压都保持在零了，可能就要改变一下权重初始化策略了：尝试用一个不那么激烈的学习率衰减，并且减少权重衰减正则化。 

 

**梯度消失/梯度爆炸** 

 

1.考虑用个好点的权重初始化策略。尤其是在，训练之初梯度就不怎么更新的情况下，这一步尤为重要。 

 

2.考虑换一下激活函数。比如ReLU，就可以拿Leaky ReLU或者MaxOut激活函数来代替。 

3.如果是RNN (递归神经网络) 的话，就可以用LSTM block。详情可参照此文：t.cn/RI6Qe7t 

 

 

 

**过拟合怎么办？** 

 

过拟合，就是神经网络记住训练数据了。如果网络在训练集和验证集上，准确度差别很大，可能它就过拟合了。详情可见 (Train/Val accuracy) ：t.cn/RAkUzJP。 

 

1.做个数据扩增。可上翻至本文第一节。 

 

2.做个Dropout。在训练的每一步，都抛弃一些神经元。详情请见：t.cn/RkZodZo。 

3.增加正则化。 

 

4.做个批量归一化。详情请见：t.cn/RNunyfR。 

 

5.做个早停止 (early stopping) 。因为，过拟合可能是训练了太多Epoch造成的。详情可见：t.cn/RkZKjEQ。 

 

6.还不行的话，就用个小一点的网络吧。不过，没到万不得已，还是别这样。 

 

 

 

**还能调些什么？** 

 

1.考虑用个带权重的损失函数。 

 

比如，在图像的语义分割中，神经网络要给每一个像素归类。其中一些类别，可能很少有像素属于它。 

 

如果，给这些不常被光顾的类别，加个权重，mean_iou这项指标就会好一些。 

 

2.改变网络架构。之前的网络，可能太深，可能太浅了。 

 

3.考虑把几个模型集成起来用。 

 

4.用跨步卷积 (strided convolution) 来代替最大池化/平均池化。 

 

5.做个完整的超参数搜索。 

 

6.改变随机种子 (random seeds)。 

 

7.上面的步骤全都不管用的话，还是再去多找点数据吧。 







1：**优化器**。机器学习训练的目的在于更新参数，优化目标函数，常见优化器有SGD，Adagrad，Adadelta，Adam，Adamax，Nadam。其中SGD和Adam优化器是最为常用的两种优化器，SGD根据每个batch的数据计算一次局部的估计，最小化代价函数。 

学习速率决定了每次步进的大小，因此我们需要选择一个合适的学习速率进行调优。学习速率太大会导致不收敛，速率太小收敛速度慢。因此SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。 

Adam优化器结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，能够自动调整学习速率，收敛速度更快，在复杂网络中表现更优。 

2：**学习速率**。学习速率的设置第一次可以设置大一点的学习率加快收敛，后续慢慢调整；也可以采用动态变化学习速率的方式（比如，每一轮乘以一个衰减系数或者根据损失的变化动态调整学习速率）。 

3：**dropout**。数据第一次跑模型的时候可以不加dropout，后期调优的时候dropout用于防止过拟合有比较明显的效果，特别是数据量相对较小的时候。 

4：**变量初始化**。常见的变量初始化有零值初始化、随机初始化、均匀分布初始值、正态分布初始值和正交分布初始值。一般采用正态分布或均匀分布的初始化值，有的论文说正交分布的初始值能带来更好的效果。实验的时候可以才正态分布和正交分布初始值做一个尝试。 

5：**训练轮数**。模型收敛即可停止迭代，一般可采用验证集作为停止迭代的条件。如果连续几轮模型损失都没有相应减少，则停止迭代。 

6：**正则化**。为了防止过拟合，可通过加入l1、l2正则化。从公式可以看出，加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。 

7：**预训练**。对需要训练的语料进行预训练可以加快训练速度，并且对于模型最终的效果会有少量的提升，常用的预训练工具有word2vec和glove。 

8：**激活函数**。常用的激活函数为sigmoid、tanh、relu、leaky relu、elu。采用sigmoid激活函数计算量较大，而且sigmoid饱和区变换缓慢，求导趋近于0，导致梯度消失。sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。 

tanh它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。 

relu从公式上可以看出，解决了gradient vanishing问题并且计算简单更容易优化，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）；leaky relu有relu的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明leaky relu总是好于relu。 

elu也是为解决relu存在的问题而提出，elu有relu的基本所有优点，但计算量稍大，并且没有完全证明elu总是好于relu。 

9：**特征学习函数**。常用的特征学习函数有cnn、rnn、lstm、gru。cnn注重词位置上的特征，而具有时序关系的词采用rnn、lstm、gru抽取特征会更有效。gru是简化版的lstm，具有更少的参数，训练速度更快。但是对于足够的训练数据，为了追求更好的性能可以采用lstm模型。 

10：**特征抽取**。max-pooling、avg-pooling是深度学习中最常用的特征抽取方式。max-pooling是抽取最大的信息向量，然而当存在多个有用的信息向量时，这样的操作会丢失大量有用的信息。 

avg-pooling是对所有信息向量求平均，当仅仅部分向量相关而大部分向量无关时，会导致有用信息向量被噪声淹没。针对这样的情况，在有多个有用向量的情形下尽量在最终的代表向量中保留这些有用的向量信息，又想在只有一个显著相关向量的情形下直接提取该向量做代表向量，避免其被噪声淹没。那么解决方案只有：加权平均，即Attention。 

11：**每轮训练数据乱序**。每轮数据迭代保持不同的顺序，避免模型每轮都对相同的数据进行计算。 

12：**batch_size选择**。对于小数据量的模型，可以全量训练，这样能更准确的朝着极值所在的方向更新。但是对于大数据，全量训练将会导致内存溢出，因此需要选择一个较小的batch_size。 

如果这时选择batch_size为1，则此时为在线学习，每次修正方向为各自样本的梯度方向修正，难以达到收敛。batch_size增大，处理相同数据量的时间减少，但是达到相同精度的轮数增多。实际中可以逐步增大batch_size，随着batch_size增大，模型达到收敛，并且训练时间最为合适。 





1 分析问题（分类回归，有监督无监督，异常检测等） 

2 合适的算法 

3 算法实现和对比 

4 特征工程 

5 超参数优化 





early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout等。 

 

Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。也就是保存表征最好的模型。 

 

 

通俗得讲，数据机扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法： 

• 从数据源头采集更多数据 

• 复制原有数据并加上随机噪声 

• 重采样 

• 根据当前数据集估计数据分布参数，使用 该分布产生更多数据等 

 

 

L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。  

正则项是为了降低模型的复杂度，从而避免模型区过分拟合训练数据，包括噪声与异常点（outliers）。从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低，这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。 
 
从贝叶斯学派来看：加了先验，在数据少的时候，先验知识可以防止过拟合。 

 

 

Dropout 

正则是通过在代价函数后面加上正则项来防止模型过拟合的。而在神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trick） 