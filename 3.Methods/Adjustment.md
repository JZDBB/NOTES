# 调参tips

神经网络针对不同的问题有不同的解决方法： 

<img src=".\img\adjust.png" height="400px">

### **训练过程中** 

**1、使用Adam作为优化器。**Adam优化效果非常好。是首选。Tensorflow实践笔记：当保存和回复模型参数时，设置AdamOptimizer之后，一定记得设置Saver，因为Adam有些state也需要恢复（即每个weight的学习率）。 

    **2、采用ReLu作为非线性激活函数。**Relu训练速度非常快，简单，而且训练效果非常好，不存在梯度消失的问题。 

    **3、在网络的输出层不要使用激活函数。** 

    **4、在每一层都添加偏置项（bias）。**因为偏置项很重要，可以把一个平面转换成一个best-fitting position。比如，y=mx+b，b就是偏置项，它使得一条直线上线移动，以便找到最优的position。 

    **5、使用variance-scaled 初始化。**在Tensorflow中，就是这个接口：tf.contrib.layers.variance_scaling_initializer()。经验中，这种generalizes/scales比其他常用的初始化方法要好，如Gaussian，truncated normal和Xavier。 

    **6、Whiten（规范化，normalize）输入数据****。**训练时，减去输入数据的均值，然后除以输入数据的方差。模型的权重延伸和拓展的角度越小，网络学习更容易且速度越快。保持输入数据是零均值的（mean-centered）且具有恒大的方差，可以帮助实现这一点。对于所有的测试数据也需要执行这一点，所以一定要确保你的训练数据与真实数据高度相似。 

    **7、在保留输入数据dynamic range的情况下，对输入数据进行尺度变换。**这个操作与normalization相关，但应先于normalization执行。举个例子，数据X实际的变化范围是[0, 140000000]，这可以被激活函数tanh（x）或tanh（x/c）所驯服（c是一个常量，延展曲线，在输入数据的变化范围匹配（fit）输入的动态特性，即tanh函数倾斜（激活）部分）。特别是你的输入数据根本没有一个上下限变化范围时，神经网络可以在（0，1）范围内，学得更好。 

    **8、不要影响learning rate的衰减。**学习速率的衰减在SGD中很常见，而且ADAM中也会自然调整它。如果你想直接一点点的调整学习速率，比如，在训练一段时间后，减小学习速率，误差曲线可能会突然drop一点点，随后很快恢复平整。 

    **9、如果你使用的卷积层使用64或128的卷积核，这已经足够了。**对于深度网络，128已经很大了，增加更多的卷积核，并不会带来效果的提升。 

    **10、Pooling（池化）以保持转移不变性。**Pooling主要是让网络对于图片输入的“该部分”具有一个“普适的作用”。比如说，最大池化（Max pooling）可以帮助CNN对于发生旋转、偏置和特征尺度变换的输入图像，也能够具有的鲁棒性。 

  

### **神经网络诊断tips** 

    如果网络不学习（意思是：loss/accuracy在训练过程中不收敛，或者没有得到你预期的结果），尝试下面的tips： 

    **1、过拟合。**如果你的模型不学习，首先应该想到模型是否陷入了过拟合情况。在一个很小的数据集上训练模型，准确率达到了100%或99.99%，或者error接近于0。如果你的神经网络不能实现过拟合，说明你的网络结构存在一些严重的问题，但也可能不是很明显。如果你的模型在小数据上过拟合，但在大数据集上任然不收敛，试一试下面的suggestion。 

    **2、减小学习速率。**模型的学习速度会变慢，但模型可以到达一个更小的局部极小值，这个点因为之前的步长过大而跳不进来。 

    **3、增大学习速率。**这可以加速模型的训练，尽快收敛，帮助模型跳出局部极小值。尽管神经网络很快就收敛了，但是其结果并不是最好的，其“收敛”得到的结果可能会及其不稳定，即不同训练，得到结果差别很大。（使用Adam，我们发现0.001是一个很好的初始值）。 

    **4、减小（mini）batch size。**减小batch size至1可以得到模型参数调整最细粒度的变化，你可以在Tensorboard（或其他debugging/可视化工具）中观察到，确定梯度更新是否存在问题。 

    **5、去除batch normalization。**当你把batch size减小至1时，起初BN可以帮助你发现模型是否存在梯度爆炸或梯度消失的问题。曾经，我们有一个神经网络不收敛，仅当我们去除BN之后我们才发现模型的输入在第二个iteration是变成了NaN。BN是一种锦上添花的措施，它只有在你确定你的模型不存在其他问题时才能正常发挥其强大的功能。 

    **6、增大（mini-）batch size。**增加batch size是必须的，越大的batch size可以减小梯度更新的方差，使得每一轮的梯度更新更加准确。换句话说，梯度更新会沿着准确的方向移动。但是！我们不可能无限制的增加batch size，因为计算机的物理内存是有限的。经验证明，这一点没有之前提出的两个suggestion重要，即减小batch size和去除BN。 

    **7、检查你的reshape操作。**频繁的进行reshape操作（比如，改变图像X和Y的维度）可能会破环空间的局部特性（spatially local features），使得神经网络几乎不能准确学习，因为它们必须学习错误的reshape。（自然的features变得破碎不堪，因为CNN的卷积操作高度依赖这些自然情况下的局部空间特征）同时对多个图片/channels进行reshape操作时必须非常的小心，使用numpy.stack()进行正确的对齐。 

    **8、监视你loss函数的变化。**如果使用的时一个复杂的函数作为目标函数，尽量使用L1或L2约束去去简化它。我们发现，L1对于边界没有那么敏感，当模型遇到噪声训练数据时，模型参数调整没有那么剧烈。 

    **9、尽可能的进行模型训练可视化。**如果你有可视化的工具，如matplotlib、OpenCV、Tensorboard等等，尽量可视化网络中值得scale、clipping等得变化，并保证不同参数着色策略得一致性。 









完成深度学习算法时要遵循的最佳实践。 
 
1.使用适当的日志和有意义的变量名。在TensorFlow中，你可以通过名称来跟踪不同的变量，并在TensorBoard中可视化图形。最重要的是，在每个训练步骤中，你都能记录相关的值，比如：step_number、accuracy、loss、learning_rate，甚至有时候还包括一些更具体的值，比如mean_intersection_over_union。之后，就可以画出每一步的损失曲线。 
 
2.确保您的网络连接正确。使用TensorBoard或其他debug技术确保图中的每个操作的输入和输出都准确无误，还要确保在将数据和标签送入网络之前对其进行适当的预处理和配对。 
 
3.实施数据增强技术。虽然这一点并不是对所有情况都适用，不过如果你在搞图像相关的神经网络，用简单的数据增强技术处理一下图像，例如镜像、旋转、随机裁剪和重新缩放、添加噪声、弹性变形等，大部分时候出来的效果都有巨大提升。 
 
4.对所有层使用权重初始化和正则化。不要把权重初始化为相同的值，当然你要是把它们都初始化成0……那就更糟了，这可能会引入对称性，并且导致梯度消失，大多数时候都会导致糟糕的结果。 
 
一般情况下，如果你在权重初始化时遇到问题，你可以考虑在神经网络中添加批量标准化层（Batch Normalization Layer）。 

 

5.确保正则化条款不会压倒损失函数中的其他项。关闭正则化，找出损失的数量级，然后适当调整正则化权重。确保在增加正则化强度时，损失也在增加。 
 
 

6.尝试过拟合一个小数据集。关闭正则化/丢失/数据增强，拿出训练集的一小部分，让神经网络练它几个世纪，确保可以实现零损失，不然就很可能是错误的。 

 

在某些情况下，将损失驱动为零非常具有挑战性，例如，如果您的损失涉及每个像素的softmax-ed logits和ground truth labels之间的交叉熵，那么在语义分割中可能真的难以将其降低到0。相反，你应该争取达到接近100％的准确度。 

 

可以在tf.metrics.accuracy这里了解如何通过获取softmax-ed logits的argmax并将其与ground truth labels进行比较来计算。 

 

7.在过拟合上述小数据集的同时，找到合理的学习率。Yoshua Bengio的论文中给到了结论：最佳学习率通常接近最大学习率的一半，不会引起训练标准的差异，这个观察结果是设置学习率的启发。例如，从较大的学习率开始，如果训练标准发散，就用最大学习率除以3再试试，直到观察不到发散为止。 

 

8.执行梯度检查。如果您在图表中使用自定义操作，则梯度检查尤其重要。斯坦福CS231n中介绍了梯度检查的方法。 

 

 

**损失没有改善怎么办？** 

如果，训练了好几个Epoch，损失还是没有变小，甚至还越来越大，就要： 

 

1.确认你用的损失函数是合适的，你优化的张量也是对的。常用损失函数列表传送门：t.cn/RkZXji1。 

 

2.用个好点的优化器。这里也有常见优化器的列表：t.cn/RDKwbNA。 

 

3.确认变量真的在训练。要检查这个，就得看张量板的直方图。 

 

或者写个脚本，在几个不同的训练实例 (training instances) 中，算出每个张量的范数。 

如果变量没在训练，请看下节，“变量没在训练怎么办？”。 

 

4.调整初始学习率，实施适当的学习率计划。 

如果损失越来越大，可能是初始学习率太大；如果损失几乎不变，可能是初始学习率太小 

 

5.确认没有过拟合。做个学习率 vs 训练步数的曲线，如果像是抛物线，可能就过拟合了。 

 

**如果有个变量不训练：** 

 

1.确认TF把它看做可训练 (trainable) 的变量。详情可以查看TF GraphKeys：t.cn/R18Do6Y。 

 

2.确认没发生梯度消失。 

 

如果下游变量(更靠近output的变量) 训练正常，而上游变量不训练，大概就是梯度消失了。 

解决方案见下文，“梯度消失/梯度爆炸”章节。 

 

3.确认ReLU (线性整流函数) 还在放电。 

 

如果大部分神经元电压都保持在零了，可能就要改变一下权重初始化策略了：尝试用一个不那么激烈的学习率衰减，并且减少权重衰减正则化。 

 

**梯度消失/梯度爆炸** 

 

1.考虑用个好点的权重初始化策略。尤其是在，训练之初梯度就不怎么更新的情况下，这一步尤为重要。 

 

2.考虑换一下激活函数。比如ReLU，就可以拿Leaky ReLU或者MaxOut激活函数来代替。 

3.如果是RNN (递归神经网络) 的话，就可以用LSTM block。详情可参照此文：t.cn/RI6Qe7t 

 

 

 

**过拟合怎么办？** 

 

过拟合，就是神经网络记住训练数据了。如果网络在训练集和验证集上，准确度差别很大，可能它就过拟合了。详情可见 (Train/Val accuracy) ：t.cn/RAkUzJP。 

 

1.做个数据扩增。可上翻至本文第一节。 

 

2.做个Dropout。在训练的每一步，都抛弃一些神经元。详情请见：t.cn/RkZodZo。 

3.增加正则化。 

 

4.做个批量归一化。详情请见：t.cn/RNunyfR。 

 

5.做个早停止 (early stopping) 。因为，过拟合可能是训练了太多Epoch造成的。详情可见：t.cn/RkZKjEQ。 

 

6.还不行的话，就用个小一点的网络吧。不过，没到万不得已，还是别这样。 

 

 

 

**还能调些什么？** 

 

1.考虑用个带权重的损失函数。 

 

比如，在图像的语义分割中，神经网络要给每一个像素归类。其中一些类别，可能很少有像素属于它。 

 

如果，给这些不常被光顾的类别，加个权重，mean_iou这项指标就会好一些。 

 

2.改变网络架构。之前的网络，可能太深，可能太浅了。 

 

3.考虑把几个模型集成起来用。 

 

4.用跨步卷积 (strided convolution) 来代替最大池化/平均池化。 

 

5.做个完整的超参数搜索。 

 

6.改变随机种子 (random seeds)。 

 

7.上面的步骤全都不管用的话，还是再去多找点数据吧。 







1：**优化器**。机器学习训练的目的在于更新参数，优化目标函数，常见优化器有SGD，Adagrad，Adadelta，Adam，Adamax，Nadam。其中SGD和Adam优化器是最为常用的两种优化器，SGD根据每个batch的数据计算一次局部的估计，最小化代价函数。 

学习速率决定了每次步进的大小，因此我们需要选择一个合适的学习速率进行调优。学习速率太大会导致不收敛，速率太小收敛速度慢。因此SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。 

Adam优化器结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，能够自动调整学习速率，收敛速度更快，在复杂网络中表现更优。 

2：**学习速率**。学习速率的设置第一次可以设置大一点的学习率加快收敛，后续慢慢调整；也可以采用动态变化学习速率的方式（比如，每一轮乘以一个衰减系数或者根据损失的变化动态调整学习速率）。 

3：**dropout**。数据第一次跑模型的时候可以不加dropout，后期调优的时候dropout用于防止过拟合有比较明显的效果，特别是数据量相对较小的时候。 

4：**变量初始化**。常见的变量初始化有零值初始化、随机初始化、均匀分布初始值、正态分布初始值和正交分布初始值。一般采用正态分布或均匀分布的初始化值，有的论文说正交分布的初始值能带来更好的效果。实验的时候可以才正态分布和正交分布初始值做一个尝试。 

5：**训练轮数**。模型收敛即可停止迭代，一般可采用验证集作为停止迭代的条件。如果连续几轮模型损失都没有相应减少，则停止迭代。 

6：**正则化**。为了防止过拟合，可通过加入l1、l2正则化。从公式可以看出，加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。 

7：**预训练**。对需要训练的语料进行预训练可以加快训练速度，并且对于模型最终的效果会有少量的提升，常用的预训练工具有word2vec和glove。 

8：**激活函数**。常用的激活函数为sigmoid、tanh、relu、leaky relu、elu。采用sigmoid激活函数计算量较大，而且sigmoid饱和区变换缓慢，求导趋近于0，导致梯度消失。sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。 

tanh它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。 

relu从公式上可以看出，解决了gradient vanishing问题并且计算简单更容易优化，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）；leaky relu有relu的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明leaky relu总是好于relu。 

elu也是为解决relu存在的问题而提出，elu有relu的基本所有优点，但计算量稍大，并且没有完全证明elu总是好于relu。 

9：**特征学习函数**。常用的特征学习函数有cnn、rnn、lstm、gru。cnn注重词位置上的特征，而具有时序关系的词采用rnn、lstm、gru抽取特征会更有效。gru是简化版的lstm，具有更少的参数，训练速度更快。但是对于足够的训练数据，为了追求更好的性能可以采用lstm模型。 

10：**特征抽取**。max-pooling、avg-pooling是深度学习中最常用的特征抽取方式。max-pooling是抽取最大的信息向量，然而当存在多个有用的信息向量时，这样的操作会丢失大量有用的信息。 

avg-pooling是对所有信息向量求平均，当仅仅部分向量相关而大部分向量无关时，会导致有用信息向量被噪声淹没。针对这样的情况，在有多个有用向量的情形下尽量在最终的代表向量中保留这些有用的向量信息，又想在只有一个显著相关向量的情形下直接提取该向量做代表向量，避免其被噪声淹没。那么解决方案只有：加权平均，即Attention。 

11：**每轮训练数据乱序**。每轮数据迭代保持不同的顺序，避免模型每轮都对相同的数据进行计算。 

12：**batch_size选择**。对于小数据量的模型，可以全量训练，这样能更准确的朝着极值所在的方向更新。但是对于大数据，全量训练将会导致内存溢出，因此需要选择一个较小的batch_size。 

如果这时选择batch_size为1，则此时为在线学习，每次修正方向为各自样本的梯度方向修正，难以达到收敛。batch_size增大，处理相同数据量的时间减少，但是达到相同精度的轮数增多。实际中可以逐步增大batch_size，随着batch_size增大，模型达到收敛，并且训练时间最为合适。 