# 调参tips

神经网络针对不同的问题有不同的解决方法： 

<img src=".\img\adjust.png" height="400px">

## 训练步骤

1 分析问题（分类回归，有监督无监督，异常检测等） 

2 合适的算法 

3 算法实现和对比 

4 特征工程 

5 超参数优化 



## 训练开始前

**1、使用适当的日志和有意义的变量名**。在每个训练步骤中，记录相关的值，比如：step_number、accuracy、loss、learning_rate，甚至有时候还包括一些更具体的值。

**2、实施数据增强技术**。在搞图像相关的神经网络，用简单的数据增强技术处理一下图像，例如镜像、旋转、随机裁剪和重新缩放、添加噪声、随机模糊图片、弹性变形等，大部分时候出来的效果都有巨大提升。另外，还可以从数据源头采集更多数据，复制原有数据并加上随机噪声， 重采样，根据当前数据集估计数据分布参数，产生更多数据等。

**3、对所有层使用权重初始化和正则化。**一般情况下，如果你在权重初始化时遇到问题，你可以考虑在神经网络中添加批量标准化层（Batch Normalization Layer）。 

**4、Early stopping**。在每一个Epoch结束时计算validation data的accuracy，当accuracy不再提高时，就停止训练，保存表征最好的模型。 

**5、预训练**。对需要训练的语料进行预训练可以加快训练速度，并且对于模型最终的效果会有少量的提升，常用的预训练工具有word2vec和glove。 

**6、每轮训练数据乱序**。每轮数据迭代保持不同的顺序，避免模型每轮都对相同的数据进行计算。 

**7、处理数据不平衡问题**。损失函数中加入权重，欠抽样（数据集大），过抽样（数据集小）等。


## 训练过程中

**1、使用Adam作为优化器。**Adam在复杂网络的优化效果非常好。是首选。Tensorflow实践笔记：当保存和回复模型参数时，设置AdamOptimizer之后，一定记得设置Saver，因为Adam有些state也需要恢复（即每个weight的学习率）。 但是SGD往往能达到最优点，但是占用的时间长。

**2、激活函数。**Relu训练速度非常快，简单，而且训练效果非常好，不存在梯度消失的问题，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）

**3、在网络的输出层不要使用激活函数。** 

**4、在每一层都添加偏置项（bias）。**因为偏置项很重要，可以把一个平面转换成一个best-fitting position。比如，y=mx+b，b就是偏置项，它使得一条直线上线移动，以便找到最优的position。 

**5、使用variance-scaled 初始化。**经验中，generalizes/scales比其他常用的初始化方法要好，如Gaussian，truncated normal和Xavier。 在Tensorflow中，就是这个接口：tf.contrib.layers.variance_scaling_initializer()。实验的时候可以才正态分布和正交分布初始值做一个尝试。 

**6、Whiten（规范化，normalize）输入数据**。训练时，减去输入数据的均值，然后除以输入数据的方差。模型的权重延伸和拓展的角度越小，网络学习更容易且速度越快。保持输入数据是零均值的（mean-centered）且具有恒大的方差，可以帮助实现这一点。对于所有的测试数据也需要执行这一点。 

**7、在保留输入数据dynamic range的情况下，对输入数据进行尺度变换。**这个操作与normalization相关，但应先于normalization执行。特别是你的输入数据根本没有一个上下限变化范围时，神经网络可以在（0，1）范围内，学得更好。 

**8、learning rate。**可以采用动态变化learning_rate的方法。

**9、如果你使用的卷积层使用64或128的卷积核，这已经足够了。**

**10、Pooling（池化）以保持转移不变性。** max-pooling、avg-pooling是深度学习中最常用的特征抽取方式。max-pooling是抽取最大的信息向量，然而当存在多个有用的信息向量时，这样的操作会丢失大量有用的信息。 avg-pooling是对所有信息向量求平均，当仅仅部分向量相关而大部分向量无关时，会导致有用信息向量被噪声淹没。在有多个有用向量的情形下尽量在最终的代表向量中保留这些有用的向量信息，又想在只有一个显著相关向量的情形下直接提取该向量做代表向量，避免其被噪声淹没。==解决方案：加权平均，即Attention==。  

**11、dropout**。数据第一次跑模型的时候可以不加dropout，后期调优的时候dropout用于防止过拟合有比较明显的效果，特别是数据量相对较小的时候。 

**12、正则化**。为了防止过拟合，可通过加入l1、l2正则化。加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。 

**13、特征学习函数**。常用的特征学习函数有cnn、rnn、lstm、gru。cnn注重词位置上的特征，而具有时序关系的词采用rnn、lstm、gru抽取特征会更有效。gru是简化版的lstm，具有更少的参数，训练速度更快。但是对于足够的训练数据，为了追求更好的性能可以采用lstm模型。 



## **神经网络诊断tips**

网络没达到预期效果时：

    **1、减小学习速率。**模型的学习速度会变慢，但模型可以到达一个更小的局部极小值，这个点因为之前的步长过大而跳不进来。 

    **2、增大学习速率。**这可以加速模型的训练，尽快收敛，帮助模型跳出局部极小值。尽管神经网络很快就收敛了，但是其“收敛”得到的结果可能会及其不稳定，即不同训练，得到结果差别很大。（使用Adam，我们发现0.001是一个很好的初始值）。 

    **3、减小（mini）batch size。**减小batch size至1可以得到模型参数调整最细粒度的变化，你可以在Tensorboard（或其他debugging/可视化工具）中观察到，确定梯度更新是否存在问题。 

    **4、去除batch normalization。**当你把batch size减小至1时，起初BN可以帮助你发现模型是否存在梯度爆炸或梯度消失的问题。BN是一种锦上添花，它只有在你确定你的模型不存在其他问题时才能正常发挥其强大的功能。 

    **5、增大（mini-）batch size。**增加batch size是必须的。但是！我们不可能无限制的增加batch size，因为计算机的物理内存是有限的。经验证明，这一点没有之前提出的两个suggestion重要，即减小batch size和去除BN。 

    **6、检查你的reshape操作。**频繁的进行reshape操作可能会破环空间的局部特性（spatially local features），使得神经网络几乎不能准确学习。因为CNN的卷积操作高度依赖这些自然情况下的局部空间特征，同时对多个图片/channels进行reshape操作时必须非常的小心，使用numpy.stack()进行正确的对齐。 

    **7、监视你loss函数的变化。**如果使用的时一个复杂的函数作为目标函数，尽量使用L1或L2约束去去简化它。我们发现，L1对于边界没有那么敏感，当模型遇到噪声训练数据时，模型参数调整没有那么剧烈。 

    **8、尽可能的进行模型训练可视化。**如果你有可视化的工具，如matplotlib、OpenCV、Tensorboard等等，尽量可视化网络中值得scale、clipping等得变化，并保证不同参数着色策略得一致性。 

  **9、梯度消失/梯度爆炸**：（1）考虑用个好点的权重初始化策略。尤其是在，训练之初梯度就不怎么更新的情况下，这一步尤为重要。（2）考虑换一下激活函数。比如ReLU，就可以拿Leaky ReLU或者MaxOut激活函数来代替。（3）如果是RNN (递归神经网络) 的话，就可以用LSTM block。 

2.考虑换一下激活函数。比如ReLU，就可以拿Leaky ReLU或者MaxOut激活函数来代替。 



## **还能调些什么？** 

- 考虑用个带权重的损失函数。 
- 如果，给这些不常被光顾的类别，加个权重，mean_iou这项指标就会好一些。 
- 考虑把几个模型集成起来用。 
- 用跨步卷积 (strided convolution) 来代替最大池化/平均池化。 
- 做个完整的超参数搜索。
- 改变随机种子 (random seeds)。 
- 裁剪网络
- 迁移学习