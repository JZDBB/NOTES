残差为什么比直接的函数更好训练，为什么bbox的训练采用使用相对坐标比绝对坐标能得到更好的结果。 
残差网络中实际上是训练一个delt(x)=f(x)-x，可以想象成delt的函数是f(x)相较于x的梯度。 

7.在过拟合上述小数据集的同时，找到合理的学习率。Yoshua Bengio的论文中给到了结论：最佳学习率通常接近最大学习率的一半，不会引起训练标准的差异，这个观察结果是设置学习率的启发。例如，从较大的学习率开始，如果训练标准发散，就用最大学习率除以3再试试，直到观察不到发散为止。 

 
8.执行梯度检查。如果您在图表中使用自定义操作，则梯度检查尤其重要。斯坦福CS231n中介绍了梯度检查的方法。

numerous candidate object locations (often called “proposals”) 


autoML 胶囊网络

德国纽伦堡大学在2016年发表了一篇face2face的论文	

c语言中的main（）      main (argc,argv)
    C语言还规定argc(第一个形参)必须是整型变量,argv( 第二个形参)必须是指向字符串的 指针数组。



1、LSTM和GRU的区别及结构
2、模型DAMSM模块图片特征提取采用inception-v3——>更好的人脸特征提取网络。
3、注意力图谱
4、


与正常的分类问题相比，人们试图区分两个（或更多）类对象，一类分类试图描述一类对象，并将其与所有其他可能的对象区分开来。在下图中，显示了一个示例数据集，表示一组苹果和梨。对象可以通过分类器很好地分类，但右下角的异常值将被归类为梨。然后应该训练一类分类器来拒绝该对象并将其标记为异常值。

这种一流的分类可以应用于不同的问题。它可用于：

1.新颖性检测（用于检测故障的机器状态监测），

2.异常检测（如上例所示，为更自信的分类），

3.数据不均衡（医学数据分类，抽样类别差），

4.数据集比较（为可比数据再次避免训练分类器）。

通常只估计训练集的概率密度。当新对象落入某个密度阈值时，此新对象被视为异常值并被拒绝。我们提出一种不依赖于密度估计的方法。该方法的灵感来自V.Vapnik的支持向量机。它计算一个球形的决策边界，在训练对象周围具有最小的体积。此要求（以及所有对象在球体内的约束）导致描述，其中球体仅由训练集中的几个对象（称为支持对象）描述。不是存储完整的训练集，而是必须存储这么小的支持对象集。通过引入类似于支持向量机的核函数，可以使球形描述更加灵活。当使用高斯核（具有额外的自由参数s）时，获得从Parzen密度估计到原始球形描述的解决方案。还给出了用于选择s的适当值的过程，使得对于所有类型的数据，可以获得严格的描述。

这些工具现在可以回答其他问题。它可以解决分类问题，其中不同的类非常不平衡（或者其中一个类完全不存在）。这种情况发生在医疗应用中，正常，健康人群的人口远远超过异常人口。它还可以指示测试集与训练集足够相似。



改进点：1、 auto-encoder上； 2、多任务学习的辅助训练； 3、尽可能的学习主类的分布和特征。 4、不平衡数据的学习； 5、比较输入和输出的相似度判断正样本



VGG在训练阶段使用了Multi-Scale的方法做数据增强，将原始图片缩放到不同的尺寸S，然后再随机裁剪224×224的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。
实验中，作者令S在[256, 512]这个区间，使用Multi-Scale获得了多个版本的数据，并将多个版本的数据合在一起训练。
在测试时，也采用了Multi-Scale的方法，将图像scale到一个尺寸Q，并将图片输入卷积网络计算，然后再最后一个卷积层使用滑窗的方式进行分类预测，将不同窗口的分类结果平均，再将不同尺寸Q的结果平均，得到最后的结果。这样可以提高数据的利用率和预测准确率。

local convolution layer？


ALOCC几种配合训练：
	1、train VAE+D（保存D参数为D0）——> train VAE+D ——> 判别的时候用D0判别
	2、train VAE+D ——> train VAE ——> 判别的时候用D判别
	3、train VAE+D（保存D参数为D0）——> train VAE+D ——> 判别的时候用D0判别
保存threshold值的方法：

最好的方法还是采用VAE encoder出来的编码向量进行OC-SVM分类或者其他传统单分类手法。



template of bird！

1 transform -> for data arguementation
2 normalization 
3 pixelshuffle for super-resolution
4 








0 4795
1 2569
2 2395
3 4743