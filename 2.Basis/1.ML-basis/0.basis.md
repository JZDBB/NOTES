# 统计学习方法
## 监督学习
#### 方法=模型+策略+算法
- 模型
决策函数集合：$F=\{f|Y = f(X)\}$
参数空间：$F=\{f|Y = f_\theta (X), \theta \in {R^n}\}$
条件概率集合：$F=\{P|P(Y|X)\}$
参数空间：$F=\{P|P_{\theta}(Y|X), \theta \in  {R^n}\}$

- 策略
损失函数、损失函数期望、风险函数、期望风险、经验风险、经验损失。——最小化
求解最优化问题——降低损失函数或者经验风险函数
$minL(Y,  P(Y|X)) \quad or \quad min \frac{1}{N}\sum L(y_i, f(x_i))$

- 算法
学习模型的具体方法，基于训练数据集。

#### 正则化
#### 交叉验证
- K折交叉验证
随机分k份大小相同的，用k-1份训练，1份测试。最后选出k次测试模型中误差最小的模型。

#### 泛化能力
模型的泛化能力是对比泛化误差的上界。
泛化误差也就是对于特定模型的损失函数的期望。
对于泛化上界：
**样本容量增加，泛化误差趋于0，假设空间容量越大，泛化误差越大。**也就是说增加数据会使得训练结果更接近真实误差，增加假设空间容量，模型就更难学。

#### 生成和判别
生成（决策函数，条件概率分布）：朴素贝叶斯、隐马尔可夫模型
判别（由数据直接学习决策函数或概率分布）：K-近邻、SVM、决策树、逻辑回归、最大熵、感知机等。

**优缺点**
**生成方法**： 可还原出联合概率分布P(X,Y), 而判别方法不能。
生成方法的收敛速度更快， 当样本容量增加的时候， 学到的模型可以更快地收敛于真实模型； 当存在隐变量时， 仍可以使用生成方法， 而判别方法则不能用。
**判别方法**： 直接学习到条件概率或决策函数， 直接进行预测， 往往学习的准确率更高； 由于直接学习Y=f(X)或P(Y|X),可对数据进行各种程度上的抽象、 定义特征并使用特征， 因此可以简化学习过程。

#### 分类问题
#### 标注问题
输入： 观测序列， 输出： 标记序列或状态序列
#### 回归问题

## 感知机
- 概念
输入的特征空间$x$可以输出空间为$y={+1,-1}$，从输入到输出到函数为：$f(x)=sign(wx+b)$称为感知机。将数据分为正负类。
- 损失函数
误分类点到分割平面的总距离：$L(w,b)=- \sum_{x_i \in M}y_i(wx_i+b)$

- 最优化问题
随机梯度下降

## k近邻法（k-NN）
#### 概念
在训练集上选出离预测特征最相邻的k个实例，这k个实例多数属于某一类，则预测其为这一类。

**优点**：精度高、对异常值不敏感、无数据输入假定
**缺点**：计算复杂度高、空间复杂度高
**适用数据范围**：数值型和标称型

**k值的选择**
如果选择较小的K值
- “学 习” 的近似误差（approximation error)会减小， 但“学习” 的估计误差（estimation error) 会增大，
- 噪声敏感
- K值的减小就意味着整体模型变得复杂， 容易发生过 拟合.

如果选择较大的K值
- 减少学习的估计误差， 但缺点是学习的近似误差会增大.
- K值的增大 就意味着整体的模型变得简单

应用中会取一个较小的k，采用交叉验证来选去最优的k值。

#### 多数表决规则（经验风险最小化）
使选出的k个实例$f: R^n \rightarrow {c_1,...,c_k}$
要减小误分类率，也就是使得$\sum_{x_i \in N_k(x)}I(y_i=c_i)$最大。

#### k近邻实现：kd树
- 平衡kd树
- 搜索kd树

## 朴素贝叶斯
训练数据集能归纳出一个基于$P(X,Y)$的独立同分布。
先验概率分布：
$P(Y=c_k), \quad k=1,2,...,K$
条件概率分布：
$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), \quad k=1,2,...,K$
独立同分布的条件概率：
$P(X=x|Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$
贝叶斯定理：
$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$
带入得：
$P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}$

#### 贝叶斯分类器
贝叶斯分类器就是最大化：
$y=f(x)=argmaxP(Y=c_k|X=x)$
带入上面的公式（消去分母）：
$y = argmax(P(X=x|Y=c_k)P(Y=c_k))$
分类器在后验概率最大化等价于期望风险最小化。



（西瓜书）


## 决策树
模型构建（归纳）$\rightarrow$预测结果（推论）
决策树的核心在于归纳，其主要依赖于数据集。
#### 决策树归纳方法
定义：最小化误差平方最小。
$E=\sum_{<b,Vtrain(b)>\in trainingexampless}(Vtrain(b)-V(b))^2$

决策树的基本组成部分： 决策结点、 分支和叶子。

#### 特征选择
**CLS算法**

**信息增益**
熵(entropy)： 信息量大小的度量， 即表示随机变量不确定性的度量。熵越大，不确定性就越大。

#### 属性选择
