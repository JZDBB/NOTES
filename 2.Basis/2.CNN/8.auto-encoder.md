# Auto-Encoder

## BASIS

原始的 AutoEncoder 结构很简单：Input Layer、Hidden Layer、Output Layer。此网络的约束有：

1. Hidden Layer 的维度要远小于 Input Layer
2. Output 用于重构 Input，也即让误差$L(Input, \ Output)$ 最小

因此，可以用 Hidden Layer 中神经元组成的向量（ Code）来表示 Input，就达到了对 Input **压缩**的效果。AutoEncoder 的训练方式就是普通的 BP。其中，将 Input 压缩为 Code 的部分称为 encoder，将 Code 还原为 Input 的部分称为 decoder。事实上，AutoEncoder 其实是**增强的 PCA**：AutoEncoder 具有非线性变换单元，因此学出来的 Code 可能更精炼，对 Input 的表达能力更强。

虽然用 AutoEncoder 来压缩理论上看起来很智能，但是实际上并不太好用：

1. 由于 AutoEncoder 是训练出来的，故它的压缩能力仅适用于与训练样本相似的样本
2. AutoEncoder 还要求 encoder 和 decoder 的能力不能太强。极端情况下，它们有能力完全记忆住训练样本，由此失去本来的压缩功能。

Autoencoder 期望利用样本自适应学习出稳健、表达能力强、扩展能力强的 Code 的设想很好，但是实际中应用场景却很有限。一般可以用于数据的降维、或者辅助进行数据的可视化分析。

## EXPAND

### **Sparse AutoEncoder**

 **Sparse Representation** —— Sparse AutoEncoder（SAE）是对 AutoEncoder 的 Code 增加了稀疏的约束。稀疏具有很多良好的性质，如：

- 有**降维**的效果，可以用于提取主要特征
- 由于可以抓住主要特征，故具有一定**抗噪**能力
- 稀疏的**可解释性好**，现实场景大多满足这种约束（如“奥卡姆剃刀定律”）

增加了稀疏约束后的 AutoEncoder 的损失函数定义如下：
$$
L_{sparse}=L+ \beta \sum_{j}KL(\rho||\widehat\rho_j)​
$$
其中，$KL$表示 KL散度，$\rho$表示网络中神经元的期望激活程度（若 Activation 为 Sigmoid 函数，此值可设为 0.05，表示大部分神经元未激活），$\rho_j$表示第$j$个神经元的平均激活程度。在此处，KL散度 定义如下:
$$
KL(\rho||\widehat\rho_j)=\rho log\frac{\rho}{\widehat\rho_j}+(1-\rho) log\frac{1-\rho}{1-\widehat\rho_j}
$$
其中，$\widehat\rho_j$定义为训练样本集上的平均激活程度，公式如下。其中$x^{(i)}$表示第$i$个训练样本
$$
\widehat\rho_j=\frac{1}{m}\sum_i[a_j(x^{(i)})]
$$

### **Denoising AutoEncoder**

Denoising AutoEncoder（DAE）是在“Vincent Extracting and composing robust features with denoising autoencoders”中提出的，本质就是在原样本中增加噪声，并期望利用 DAE 将加噪样本来还原成纯净样本。以图像为例，对图像中的像素以一定概率遮挡，作为输入。随后利用 DAE 进行恢复。由于增加了噪声，因此学习出来的 Code 会更加稳健。目前应用不多。

### **Variational AutoEncoder**

